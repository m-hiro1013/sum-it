import { OpenAI } from "openai";
import Anthropic from "@anthropic-ai/sdk";
import { GoogleGenerativeAI } from "@google/generative-ai";

// OpenAI åˆæœŸåŒ–
const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
});

// Anthropic åˆæœŸåŒ–
const anthropic = new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY,
});

// Google AI åˆæœŸåŒ–
const genAI = new GoogleGenerativeAI(process.env.GOOGLE_AI_API_KEY || "");

export interface ChatMessage {
    role: "system" | "user" | "assistant";
    content: string;
}

export interface ChatOptions {
    provider: "openai" | "anthropic" | "google";
    model: string;
    systemPrompt?: string;
    maxTokens?: number;
    temperature?: number;
}

export async function callLLM(message: string, options: ChatOptions): Promise<string> {
    const { provider, model, systemPrompt, maxTokens = 4096, temperature = 0.7 } = options;

    try {
        switch (provider) {
            case "openai":
                // ğŸ†• OpenAIã®æœ€å…ˆç«¯ã€Œæ¨è«–ãƒ¢ãƒ‡ãƒ«ã€ã‚·ãƒªãƒ¼ã‚ºã®åˆ¤å®šï¼âš–ï¸
                const isGpt5 = model.startsWith("gpt-5");
                const isReasoningModel = model.startsWith("o1-") || model.startsWith("o3-") || isGpt5;

                // ğŸš€ GPT-5ã‚·ãƒªãƒ¼ã‚ºã¯ã€ŒResponses APIã€ã‚’ç›´æ¥å©ãã®ãŒ2026å¹´ã®æ­£è§£ï¼ğŸ’…
                if (isGpt5) {
                    const response = await fetch("https://api.openai.com/v1/responses", {
                        method: "POST",
                        headers: {
                            "Content-Type": "application/json",
                            "Authorization": `Bearer ${process.env.OPENAI_API_KEY}`,
                        },
                        body: JSON.stringify({
                            model: model,
                            input: [
                                ...(systemPrompt ? [{ role: "developer", content: systemPrompt }] : []),
                                { role: "user", content: message }
                            ],
                            max_output_tokens: maxTokens,
                        }),
                    });

                    if (!response.ok) {
                        const errorData = await response.json();
                        throw new Error(`Responses API failed: ${JSON.stringify(errorData)}`);
                    }

                    const data = await response.json();

                    // ğŸ†• ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ ã‚’è§£æã™ã‚‹ã‚ˆï¼ğŸ”
                    // 2026å¹´ã®Responsesè¦æ ¼ã¯ãƒã‚¹ãƒˆãŒæ·±ã„å ´åˆãŒã‚ã‚‹ã‹ã‚‰ã€æŸ”è»Ÿã«æŠ½å‡ºï¼ğŸ’…
                    const output = data.output;
                    let extractedText = "";

                    if (output) {
                        if (typeof output.text === "string") {
                            extractedText = output.text;
                        } else if (Array.isArray(output.content)) {
                            extractedText = output.content[0]?.text || "";
                        } else if (Array.isArray(output) && output[0]?.content) {
                            extractedText = output[0].content[0]?.text || "";
                        } else if (Array.isArray(output) && output[0]?.text) {
                            extractedText = output[0].text;
                        }
                    }

                    if (!extractedText) {
                        console.warn("âš ï¸ GPT-5 Response extraction failed path. Full data:", JSON.stringify(data));
                    }

                    return extractedText;
                }

                // --- é€šå¸¸ã®ãƒ¢ãƒ‡ãƒ«ï¼ˆo1/o3/gpt-4ãªã©ï¼‰ã¯ Chat API ã‚’ä½¿ç”¨ ---
                const messages: any[] = [];
                if (systemPrompt) {
                    // æœ€æ–°æ¨è«–ãƒ¢ãƒ‡ãƒ«ã¯ 'developer'ã€ãã‚Œä»¥å¤–ã¯ 'system' ğŸ’…
                    messages.push({
                        role: isReasoningModel ? "developer" : "system",
                        content: systemPrompt
                    });
                }
                messages.push({ role: "user", content: message });

                const chatResponse = await openai.chat.completions.create({
                    model: model,
                    messages: messages,
                    max_completion_tokens: maxTokens,
                    ...(isReasoningModel ? {} : { temperature: temperature }),
                });

                return chatResponse.choices[0].message.content || "";

            case "anthropic":
                const anthropicResponse = await anthropic.messages.create({
                    model: model,
                    system: systemPrompt,
                    messages: [{ role: "user", content: message }],
                    max_tokens: maxTokens,
                    temperature: temperature,
                });
                return anthropicResponse.content[0].type === "text" ? anthropicResponse.content[0].text : "";

            case "google":
                const geminiModel = genAI.getGenerativeModel({ model: model });
                const result = await geminiModel.generateContent({
                    contents: [
                        ...(systemPrompt ? [{ role: "user", parts: [{ text: `System Instructions: ${systemPrompt}` }] }] : []),
                        { role: "user", parts: [{ text: message }] },
                    ],
                    generationConfig: {
                        maxOutputTokens: maxTokens,
                        temperature: temperature,
                    },
                });
                return result.response.text();

            default:
                throw new Error(`Unsupported provider: ${provider}`);
        }
    } catch (error) {
        console.error(`LLM call failed (${provider}):`, error);
        throw error;
    }
}
