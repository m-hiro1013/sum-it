import { OpenAI } from "openai";
import Anthropic from "@anthropic-ai/sdk";
import { GoogleGenerativeAI } from "@google/generative-ai";

// OpenAI åˆæœŸåŒ–
const openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
});

// Anthropic åˆæœŸåŒ–
const anthropic = new Anthropic({
    apiKey: process.env.ANTHROPIC_API_KEY,
});

// Google AI åˆæœŸåŒ–
const genAI = new GoogleGenerativeAI(process.env.GOOGLE_AI_API_KEY || "");

export interface ChatMessage {
    role: "system" | "user" | "assistant";
    content: string;
}

export interface ChatOptions {
    provider: "openai" | "anthropic" | "google";
    model: string;
    systemPrompt?: string;
    cacheableContext?: string;  // ğŸ†• è¿½åŠ ï¼šã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆï¼ˆwhiteboardç­‰ï¼‰ğŸ’…
    maxTokens?: number;
    temperature?: number;
}

export interface LLMResponse {
    content: string;
    usage: {
        input_tokens: number;
        output_tokens: number;
    };
}

/**
 * ğŸ†• æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ä»˜ããƒªãƒˆãƒ©ã‚¤é–¢æ•°ï¼ğŸ›¡ï¸
 */
async function withRetry<T>(
    fn: () => Promise<T>,
    maxRetries = 3,
    initialDelay = 1000
): Promise<T> {
    let lastError: any;
    for (let i = 0; i <= maxRetries; i++) {
        try {
            return await fn();
        } catch (error: any) {
            lastError = error;

            // ãƒªãƒˆãƒ©ã‚¤ã™ã¹ãã‚¨ãƒ©ãƒ¼ã‹åˆ¤å®šï¼ˆ429: Rate Limit, 503: Service Unavailable ãªã©ï¼‰ğŸ’…
            const status = error.status || error.statusCode || (error.response?.status);
            const shouldRetry = status === 429 || status === 503 || (error.message && (
                error.message.includes("rate limit") ||
                error.message.includes("timeout") ||
                error.message.includes("unavailable")
            ));

            if (!shouldRetry || i === maxRetries) break;

            const delay = initialDelay * Math.pow(2, i);
            console.warn(`âš ï¸ LLMãƒªãƒˆãƒ©ã‚¤ä¸­ (${i + 1}/${maxRetries}): ${delay}mså¾…æ©Ÿ...`);
            await new Promise(resolve => setTimeout(resolve, delay));
        }
    }
    throw lastError;
}

// çµ±åˆãƒ­ã‚°å‡ºåŠ›ç”¨ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ğŸ’…
interface LLMCacheLog {
    provider: string;
    model: string;
    hasCacheableContext: boolean;
    cacheableContextLength: number;
    cacheCreationTokens?: number;  // Claudeç”¨
    cacheReadTokens?: number;      // Claudeç”¨
    cachedContentTokens?: number;  // Geminiç”¨
    cachedTokens?: number;         // OpenAIç”¨
    inputTokens: number;
    outputTokens: number;
}

function logCacheInfo(log: LLMCacheLog): void {
    const isDev = process.env.NODE_ENV === "development";
    const cacheHit = log.cacheReadTokens || log.cachedContentTokens || log.cachedTokens || 0;

    if (isDev) {
        console.log(`[LLM Cache Detail] ${log.provider}/${log.model}`, JSON.stringify(log, null, 2));
    }
    // æœ¬ç•ªãƒ»é–‹ç™ºå…±é€šã®ã‚µãƒãƒªãƒ¼ãƒ­ã‚°
    console.log(`ğŸ“¡ [LLM Cache] ${log.provider}/${log.model} - CacheHit: ${cacheHit > 0 ? "âœ… YES" : "âŒ NO"} (${cacheHit} tokens)`);
}

export async function callLLM(message: string, options: ChatOptions): Promise<LLMResponse> {
    const {
        provider,
        model,
        systemPrompt,
        cacheableContext,
        maxTokens = 4096,
        temperature = 0.7
    } = options;
    const timeout = 60000; // 60ç§’ã§ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆğŸ’…
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), timeout);

    try {
        return await withRetry(async () => {
            switch (provider) {
                case "openai":
                    const isGpt5 = model.startsWith("gpt-5");
                    const isReasoningModel = model.startsWith("o1-") || model.startsWith("o3-") || isGpt5;

                    if (isGpt5) {
                        const input: any[] = [];
                        // 1. ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡ã‚’å…ˆé ­ã«é…ç½®ï¼ˆOpenAIã¯å…ˆé ­ä¸€è‡´ã§è‡ªå‹•ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
                        if (cacheableContext) {
                            input.push({ role: "developer", content: `## å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\n\n${cacheableContext}` });
                        }
                        if (systemPrompt) {
                            input.push({ role: "developer", content: systemPrompt });
                        }
                        input.push({ role: "user", content: message });

                        const response = await fetch("https://api.openai.com/v1/responses", {
                            method: "POST",
                            headers: {
                                "Content-Type": "application/json",
                                "Authorization": `Bearer ${process.env.OPENAI_API_KEY}`,
                            },
                            body: JSON.stringify({
                                model: model,
                                input: input,
                                max_output_tokens: maxTokens,
                            }),
                            signal: controller.signal,
                        });

                        if (!response.ok) {
                            const errorData = await response.json();
                            throw new Error(`Responses API failed: ${JSON.stringify(errorData)}`);
                        }

                        const data = await response.json();
                        const resultText = data.output?.text || (Array.isArray(data.output) ? data.output[0]?.text : "");

                        logCacheInfo({
                            provider: "openai-responses",
                            model,
                            hasCacheableContext: !!cacheableContext,
                            cacheableContextLength: cacheableContext?.length || 0,
                            cachedTokens: data.usage?.prompt_tokens_details?.cached_tokens || 0,
                            inputTokens: data.usage?.input_tokens || 0,
                            outputTokens: data.usage?.output_tokens || 0,
                        });

                        return {
                            content: resultText,
                            usage: {
                                input_tokens: data.usage?.input_tokens || 0,
                                output_tokens: data.usage?.output_tokens || 0,
                            }
                        };
                    }

                    const messages: any[] = [];
                    if (cacheableContext) {
                        messages.push({
                            role: isReasoningModel ? "developer" : "system",
                            content: `## å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\n\n${cacheableContext}`
                        });
                    }
                    if (systemPrompt) {
                        messages.push({
                            role: isReasoningModel ? "developer" : "system",
                            content: systemPrompt
                        });
                    }
                    messages.push({ role: "user", content: message });

                    const chatResponse = await openai.chat.completions.create({
                        model: model,
                        messages: messages,
                        max_completion_tokens: maxTokens,
                        ...(isReasoningModel ? {} : { temperature: temperature }),
                    }, { signal: controller.signal });

                    logCacheInfo({
                        provider: "openai",
                        model,
                        hasCacheableContext: !!cacheableContext,
                        cacheableContextLength: cacheableContext?.length || 0,
                        cachedTokens: (chatResponse.usage as any)?.prompt_tokens_details?.cached_tokens || 0,
                        inputTokens: chatResponse.usage?.prompt_tokens || 0,
                        outputTokens: chatResponse.usage?.completion_tokens || 0,
                    });

                    return {
                        content: chatResponse.choices[0].message.content || "",
                        usage: {
                            input_tokens: chatResponse.usage?.prompt_tokens || 0,
                            output_tokens: chatResponse.usage?.completion_tokens || 0,
                        }
                    };

                case "anthropic":
                    type SystemBlock = { type: "text"; text: string; cache_control?: { type: "ephemeral" } };
                    const systemBlocks: SystemBlock[] = [];

                    // 1. ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡ã‚’å…ˆé ­ã«é…ç½®ï¼ˆcache_control: ephemeral ã‚’æ˜ç¤ºï¼‰ğŸ’…
                    if (cacheableContext) {
                        systemBlocks.push({
                            type: "text",
                            text: `## å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\n\n${cacheableContext}`,
                            cache_control: { type: "ephemeral" }
                        });
                    }
                    if (systemPrompt) {
                        systemBlocks.push({ type: "text", text: systemPrompt });
                    }

                    const anthropicResponse = await anthropic.messages.create({
                        model: model,
                        system: systemBlocks.length > 0 ? systemBlocks : undefined,
                        messages: [{ role: "user", content: message }],
                        max_tokens: maxTokens,
                        temperature: temperature,
                    }, { signal: controller.signal });

                    logCacheInfo({
                        provider: "anthropic",
                        model,
                        hasCacheableContext: !!cacheableContext,
                        cacheableContextLength: cacheableContext?.length || 0,
                        cacheCreationTokens: (anthropicResponse.usage as any).cache_creation_input_tokens || 0,
                        cacheReadTokens: (anthropicResponse.usage as any).cache_read_input_tokens || 0,
                        inputTokens: anthropicResponse.usage.input_tokens,
                        outputTokens: anthropicResponse.usage.output_tokens,
                    });

                    return {
                        content: anthropicResponse.content[0].type === "text" ? anthropicResponse.content[0].text : "",
                        usage: {
                            input_tokens: anthropicResponse.usage.input_tokens,
                            output_tokens: anthropicResponse.usage.output_tokens,
                        }
                    };

                case "google":
                    const geminiModel = genAI.getGenerativeModel({
                        model: model,
                        systemInstruction: systemPrompt || undefined,
                    });

                    const contents: any[] = [];
                    // 1. ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡ã‚’å…ˆé ­ã«é…ç½®ï¼ˆGeminiã¯æš—é»™çš„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
                    if (cacheableContext) {
                        contents.push({
                            role: "user",
                            parts: [{ text: `## å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ\n\nä»¥ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦è­°è«–ã«è²¢çŒ®ã—ã¦ãã ã•ã„ï¼š\n\n${cacheableContext}` }]
                        });
                        contents.push({ role: "model", parts: [{ text: "ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèªã—ã¾ã—ãŸã€‚å†…å®¹ã‚’æŠŠæ¡ã—ãŸä¸Šã§å›ç­”ã—ã¾ã™ã€‚" }] });
                    }
                    contents.push({ role: "user", parts: [{ text: message }] });

                    const result = await geminiModel.generateContent({
                        contents: contents,
                        generationConfig: {
                            maxOutputTokens: maxTokens,
                            temperature: temperature,
                        },
                    });

                    const usage = result.response.usageMetadata;
                    logCacheInfo({
                        provider: "google",
                        model,
                        hasCacheableContext: !!cacheableContext,
                        cacheableContextLength: cacheableContext?.length || 0,
                        cachedContentTokens: usage?.cachedContentTokenCount || 0,
                        inputTokens: usage?.promptTokenCount || 0,
                        outputTokens: usage?.candidatesTokenCount || 0,
                    });

                    return {
                        content: result.response.text(),
                        usage: {
                            input_tokens: usage?.promptTokenCount || 0,
                            output_tokens: usage?.candidatesTokenCount || 0,
                        }
                    };

                default:
                    throw new Error(`Unsupported provider: ${provider}`);
            }
        });
    } catch (error: any) {
        if (error.name === "AbortError") {
            console.error(`âŒ LLMãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸ (${timeout}ms):`, provider);
            throw new Error(`LLMãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã—ã¾ã—ãŸã€‚ã—ã°ã‚‰ãå¾…ã£ã¦ã‹ã‚‰å†è©¦è¡Œã—ã¦ãã ã•ã„ã€‚ğŸ’…`);
        }
        console.error(`LLM call failed (${provider}):`, error);
        throw error;
    } finally {
        clearTimeout(timeoutId);
    }
}


