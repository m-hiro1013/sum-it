import { callLLM } from "./llm";
import { Agent } from "../types/agent";
import { Meeting, Message } from "../types/meeting";
import { Facilitator } from "../types/facilitator";
import { MeetingWorkflow, WorkflowStep, SpeakStep, ParallelSpeakStep, SummaryStep, UserInterventionStep } from "../types/workflow";
import { OutputStyle } from "../types/style";
import { getOutputStyle } from "./firestore";

// ==========================================
// å‹å®šç¾©
// ==========================================

export interface ExecutionContext {
    meeting: Meeting;
    workflow: MeetingWorkflow;
    facilitator: Facilitator;
    agents: Map<string, Agent>;      // agent_id -> Agent
    messages: Message[];             // ã“ã‚Œã¾ã§ã®å…¨ç™ºè¨€
    whiteboard: string;              // ç¾åœ¨ã®ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰
}

export interface StepResult {
    content: string;
    agent_id: string;
    agent_name: string;
}

export interface ExecutionResult {
    success: boolean;
    status: "in_progress" | "waiting" | "completed";
    messages: StepResult[];
    error?: string;
}

// ==========================================
// ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
// ==========================================

export async function executeNextStep(
    context: ExecutionContext
): Promise<ExecutionResult> {
    const { workflow, meeting } = context;
    const currentStep = workflow.steps[meeting.current_step || 0];

    if (!currentStep) {
        return {
            success: true,
            status: "completed",
            messages: [],
        };
    }

    switch (currentStep.type) {
        case "speak":
            return await handleSpeak(currentStep, context);

        case "parallel_speak":
            return await handleParallelSpeak(currentStep, context);

        case "summary":
            return await handleSummary(currentStep, context);

        case "user_intervention":
            return await handleUserIntervention(currentStep, context);

        default:
            return {
                success: false,
                status: "in_progress",
                messages: [],
                error: `Unknown step type: ${(currentStep as any).type}`,
            };
    }
}

// ==========================================
// å„ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
// ==========================================

/**
 * ğŸ†• æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ1äººãŒç™ºè¨€ã™ã‚‹
 */
async function handleSpeak(
    step: SpeakStep,
    context: ExecutionContext
): Promise<ExecutionResult> {
    const { agents, meeting, messages, whiteboard, facilitator } = context;

    // 1. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå–å¾—
    const agent = agents.get(step.agent_id);
    if (!agent) {
        return {
            success: false,
            status: "in_progress",
            messages: [],
            error: `Agent not found in execution context: ${step.agent_id}`,
        };
    }

    // 2. å‡ºåŠ›ã‚¹ã‚¿ã‚¤ãƒ«å–å¾—
    const style = await getOutputStyle(agent.style_id);
    if (!style) {
        return {
            success: false,
            status: "in_progress",
            messages: [],
            error: `Output style not found: ${agent.style_id}`,
        };
    }

    // 3. ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    const systemPrompt = buildSystemPrompt(agent, style, facilitator);

    // 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
    const userMessage = buildUserMessage(
        meeting.topic,
        whiteboard,
        messages,
        agent.role
    );

    try {
        // 5. LLMå‘¼ã³å‡ºã—
        const response = await callLLM(userMessage, {
            provider: agent.llm as "openai" | "anthropic" | "google",
            model: agent.model,
            systemPrompt: systemPrompt,
        });

        // 6. çµæœã‚’è¿”ã™
        return {
            success: true,
            status: "in_progress",
            messages: [{
                content: response,
                agent_id: agent.id,
                agent_name: agent.name,
            }],
        };
    } catch (error: any) {
        console.error("LLM Execution Error:", error);
        return {
            success: false,
            status: "in_progress",
            messages: [],
            error: `LLM call failed: ${error.message || "Unknown error"}`,
        };
    }
}

/**
 * ğŸ†• è¤‡æ•°ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåŒæ™‚ã«ç™ºè¨€ã™ã‚‹
 */
async function handleParallelSpeak(
    step: ParallelSpeakStep,
    context: ExecutionContext
): Promise<ExecutionResult> {
    const { agents, meeting, messages, whiteboard, facilitator } = context;

    try {
        const promises = step.agent_ids.map(async (agentId) => {
            const agent = agents.get(agentId);
            if (!agent) {
                throw new Error(`Agent not found: ${agentId}`);
            }

            const style = await getOutputStyle(agent.style_id);
            if (!style) {
                throw new Error(`Output style not found for agent ${agent.name}`);
            }

            const systemPrompt = buildSystemPrompt(agent, style, facilitator);
            const userMessage = buildUserMessage(
                meeting.topic,
                whiteboard,
                messages,
                agent.role
            );

            const response = await callLLM(userMessage, {
                provider: agent.llm as "openai" | "anthropic" | "google",
                model: agent.model,
                systemPrompt: systemPrompt,
            });

            return {
                content: response,
                agent_id: agent.id,
                agent_name: agent.name,
            };
        });

        const results = await Promise.all(promises);

        return {
            success: true,
            status: "in_progress",
            messages: results,
        };
    } catch (error: any) {
        console.error("Parallel LLM Execution Error:", error);
        return {
            success: false,
            status: "in_progress",
            messages: [],
            error: `Parallel LLM call failed: ${error.message || "Unknown error"}`,
        };
    }
}

/**
 * ğŸ†• è­°é•·ãŒã“ã‚Œã¾ã§ã®è­°è«–ã‚’ã¾ã¨ã‚ã‚‹
 */
async function handleSummary(
    _step: SummaryStep,
    context: ExecutionContext
): Promise<ExecutionResult> {
    const { meeting, messages, whiteboard, facilitator } = context;

    // 1. ã“ã‚Œã¾ã§ã®å…¨ç™ºè¨€ã‚’æ•´å½¢
    const history = messages.length > 0
        ? messages.map(m => `ã€${m.agent_name}ã€‘\n${m.content}`).join("\n\n")
        : "ï¼ˆè­°è«–ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã§ã—ãŸï¼‰";

    // 2. ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè­°é•·å°‚ç”¨ğŸ’…ï¼‰
    const systemPrompt = `ã‚ãªãŸã¯ä¼šè­°ã®è­°é•·ï¼ˆãƒ•ã‚¡ã‚·ãƒªãƒ†ãƒ¼ã‚¿ãƒ¼ï¼‰ã§ã™ã€‚
ä»¥ä¸‹ã®æŒ‡ç¤ºã«å¾“ã£ã¦ã€ã“ã‚Œã¾ã§ã®è­°è«–ã‚’è«–ç†çš„ã‹ã¤å»ºè¨­çš„ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

## è­°é•·ã¨ã—ã¦ã®æŒ‡ç¤º
${facilitator.end_prompt}

---
å¸¸ã«ä¸­ç«‹ã§ã€ã‹ã¤æ¬¡ã«ç¹‹ãŒã‚‹å‰å‘ããªã¾ã¨ã‚ã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚`;

    // 3. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    const userMessage = `## ä¼šè­°ã®è­°é¡Œ
${meeting.topic}

## ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰ï¼ˆã“ã‚Œã¾ã§ã®åˆæ„äº‹é …ãƒ»å…±æœ‰æƒ…å ±ï¼‰
${whiteboard || "ï¼ˆç‰¹ã«ãªã—ï¼‰"}

## ã“ã‚Œã¾ã§ã®ã™ã¹ã¦ã®ç™ºè¨€å±¥æ­´
${history}

---
ä¸Šè¨˜ã®è­°è«–ã‚’è¸ã¾ãˆã¦ã€è­°é•·ã¨ã—ã¦ã€Œçµè«–ã‚µãƒãƒªãƒ¼ã€ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚`;

    try {
        // 4. LLMå‘¼ã³å‡ºã—ï¼ˆè­°é•·ã¯å®‰å®šã® GPT-4o ã‚’ä½¿ç”¨ï¼‰
        const response = await callLLM(userMessage, {
            provider: "openai",
            model: "gpt-4o",
            systemPrompt: systemPrompt,
        });

        return {
            success: true,
            status: "completed", // ã‚µãƒãƒªãƒ¼ãŒå‡ºãŸã‚‰ä¼šè­°çµ‚äº†ï¼ğŸ
            messages: [{
                content: response,
                agent_id: "facilitator",
                agent_name: `è­°é•·ï¼ˆ${facilitator.name}ï¼‰`,
            }],
        };
    } catch (error: any) {
        console.error("Summary LLM Execution Error:", error);
        return {
            success: false,
            status: "in_progress",
            messages: [],
            error: `Summary generation failed: ${error.message || "Unknown error"}`,
        };
    }
}

/**
 * ğŸ†• ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»‹å…¥ã‚’å¾…ã¤ï¼ˆä¸€æ™‚åœæ­¢ï¼‰
 */
async function handleUserIntervention(
    step: UserInterventionStep,
    _context: ExecutionContext
): Promise<ExecutionResult> {
    // ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯LLMã¯å‘¼ã°ãšã€å˜ã«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ "waiting" ã«ã—ã¦è¿”ã™ã ã‘ï¼
    // å®Ÿéš›ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦ã€Œå¾…æ©Ÿä¸­ã€ã¨ã„ã†ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºã™ã‚ˆã€‚

    return {
        success: true,
        status: "waiting", // ã“ã‚ŒãŒè¶…å¤§äº‹ï¼ğŸ’…
        messages: [{
            content: step.label || "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å¾…ã£ã¦ã„ã¾ã™... ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰ã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚ğŸ’…âœ¨",
            agent_id: "system",
            agent_name: "ã‚·ã‚¹ãƒ†ãƒ ",
        }],
    };
}

// ==========================================
// ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆã‚®ãƒ£ãƒ«ã®ã“ã ã‚ã‚Šï¼ğŸ’…ï¼‰
// ==========================================

function buildSystemPrompt(
    agent: Agent,
    style: OutputStyle,
    facilitator: Facilitator
): string {
    return `ã‚ãªãŸã¯ã€Œ${agent.name}ã€ã¨ã„ã†åå‰ã®ä¼šè­°å‚åŠ è€…ã§ã™ã€‚

## ã‚ãªãŸã®å½¹å‰²
${agent.role}

## ã‚ãªãŸã®æ€§æ ¼ãƒ»è¨­å®š
${agent.persona}

${agent.prompt ? `## è¿½åŠ ã®æŒ‡ç¤º\n${agent.prompt}` : ""}

## è­°é•·ã‹ã‚‰ã®å…¨ä½“æŒ‡ç¤º
${facilitator.start_prompt}

## å‡ºåŠ›å½¢å¼ãƒ»ã‚¹ã‚¿ã‚¤ãƒ«
${style.prompt_segment}

---
ä¸Šè¨˜ã®è¨­å®šã‚’éµå®ˆã—ã¦ã€è­°è«–ã«è²¢çŒ®ã—ã¦ãã ã•ã„ã€‚`;
}

function buildUserMessage(
    topic: string,
    whiteboard: string,
    messages: Message[],
    role: string
): string {
    // ã“ã‚Œã¾ã§ã®ç™ºè¨€ã‚’æ•´å½¢ï¼ˆCONTEXT!ï¼‰
    const history = messages.length > 0
        ? messages.map(m => `ã€${m.agent_name}ã€‘\n${m.content}`).join("\n\n")
        : "ï¼ˆã¾ã ç™ºè¨€ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰";

    return `## ä¼šè­°ã®è­°é¡Œ
${topic}

## ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰ï¼ˆã“ã‚Œã¾ã§ã®åˆæ„äº‹é …ãƒ»å…±æœ‰æƒ…å ±ï¼‰
${whiteboard || "ï¼ˆç‰¹ã«æ›¸ãè¾¼ã¿ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰"}

## è­°è«–ã®å±¥æ­´
${history}

---
ã‚ãªãŸã¯ã€Œ${role}ã€ã¨ã—ã¦ã€ã“ã®è­°è«–ã®æµã‚Œã‚’è¸ã¾ãˆã€æ¬¡ã«è¿°ã¹ã‚‹ã¹ãæ„è¦‹ã‚„è³ªå•ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚`;
}
