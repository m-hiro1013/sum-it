import { callLLM } from "./llm";
import { Agent } from "../types/agent";
import { Meeting, Message } from "../types/meeting";
import { MeetingWorkflow, WorkflowStep, SpeakStep, ParallelSpeakStep, SummaryStep, UserInterventionStep } from "../types/workflow";
import { OutputStyle } from "../types/style";
import { getOutputStyle } from "./firestore";
import { GoogleGenerativeAI } from "@google/generative-ai"; // ğŸ†• ã“ã‚Œå¿˜ã‚Œã¦ãŸï¼

// ==========================================
// å‹å®šç¾©
// ==========================================

export interface ExecutionContext {
    meeting: Meeting;
    workflow: MeetingWorkflow;
    // facilitator: Facilitator;      // âŒ å‰Šé™¤
    agents: Map<string, Agent>;      // agent_id -> Agent
    messages: Message[];             // ã“ã‚Œã¾ã§ã®å…¨ç™ºè¨€
    whiteboard: string;              // ç¾åœ¨ã®ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰
}

export interface StepResult {
    content: string;
    agent_id: string;
    agent_name: string;
    agent_role: string;
    agent_avatar_url?: string;
    usage?: {
        input_tokens: number;
        output_tokens: number;
    };
}

export interface ExecutionResult {
    success: boolean;
    status: "in_progress" | "waiting" | "completed";
    messages: StepResult[];
    total_usage?: {
        input_tokens: number;
        output_tokens: number;
    };
    error?: string;
}

/**
 * ğŸ†• ãƒ¢ãƒ‡ãƒ«åã‚’APIãŒå—ã‘å–ã‚Œã‚‹å½¢å¼ã«å¤‰æ›ã™ã‚‹ã‚ˆï¼ğŸ’…
 */
function normalizeModelId(modelId: string): string {
    const mapping: Record<string, string> = {
        // --- Google Gemini (2026 Feb æœ€æ–°ä»•æ§˜) ---
        // 404ã‚¨ãƒ©ãƒ¼ã®æ­£ä½“: ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ¢ãƒ‡ãƒ«ã¯ "-preview" ãŒå¿…é ˆã ã‚ˆğŸ’…
        "gemini-3-flash": "gemini-3-flash-preview",
        "gemini-3-pro": "gemini-3-pro-preview",
        "gemini-flash-latest": "gemini-3-flash-preview",

        // --- Anthropic Claude (2026 Feb æœ€æ–°ä»•æ§˜) ---
        // ä»•æ§˜æ›¸æº–æ‹ : è­˜åˆ¥å­ã«æ­£ç¢ºãªæ—¥ä»˜ã¾ãŸã¯æœ€æ–°ã‚¨ã‚¤ãƒªã‚¢ã‚¹ã‚’æŒ‡å®š
        "claude-4.5-sonnet": "claude-4-5-sonnet-20250929",
        "claude-4.5-opus": "claude-4-5-opus-20251101",
        "claude-4.5-haiku": "claude-4-5-haiku-20251015",

        // --- OpenAI GPT-5 (2026 Feb æœ€æ–°ä»•æ§˜) ---
        // GPT-5.2ãŒä¸»æµã ã‘ã©APIã§ã¯ chat-latest ç­‰ãŒæ¨å¥¨
        "gpt-5": "gpt-5-chat-latest",
        "gpt-5-thinking": "gpt-5",
    };
    return mapping[modelId] || modelId;
}

// ==========================================
// ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
// ==========================================

export async function executeNextStep(
    context: ExecutionContext
): Promise<ExecutionResult> {
    const { workflow, meeting } = context;
    const currentStep = workflow.steps[meeting.current_step || 0];

    if (!currentStep) {
        return {
            success: true,
            status: "completed",
            messages: [],
        };
    }

    switch (currentStep.type) {
        case "speak":
            return await handleSpeak(currentStep, context);

        case "parallel_speak":
            return await handleParallelSpeak(currentStep, context);

        case "summary":
            return await handleSummary(currentStep, context);

        case "user_intervention":
            return await handleUserIntervention(currentStep, context);

        default:
            return {
                success: false,
                status: "in_progress",
                messages: [],
                error: `Unknown step type: ${(currentStep as any).type}`,
            };
    }
}

// ==========================================
// å„ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼
// ==========================================

/**
 * ğŸ†• æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ1äººãŒç™ºè¨€ã™ã‚‹
 */
async function handleSpeak(
    step: SpeakStep,
    context: ExecutionContext
): Promise<ExecutionResult> {
    const { agents, meeting, messages, whiteboard, workflow } = context;

    // 1. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå–å¾—
    const agent = agents.get(step.agent_id);
    if (!agent) {
        return {
            success: false,
            status: "in_progress",
            messages: [],
            error: `Agent not found in execution context: ${step.agent_id}`,
        };
    }

    // 2. å‡ºåŠ›ã‚¹ã‚¿ã‚¤ãƒ«å–å¾—
    const style = await getOutputStyle(agent.style_id);
    if (!style) {
        return {
            success: false,
            status: "in_progress",
            messages: [],
            error: `Output style not found: ${agent.style_id}`,
        };
    }

    // 3. ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
    const startPrompt = meeting.start_prompt_override || workflow.start_prompt;
    const systemPrompt = buildSystemPrompt(agent, style, startPrompt);

    // 4. ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ§‹ç¯‰
    const userMessage = buildUserMessage(
        meeting.topic,
        messages, // ğŸ’… whiteboardã‚’å‰Šé™¤
        agent.role
    );

    try {
        // 5. LLMå‘¼ã³å‡ºã—
        const response = await callLLM(userMessage, {
            provider: agent.llm as "openai" | "anthropic" | "google",
            model: normalizeModelId(agent.model),
            systemPrompt: systemPrompt,
            cacheableContext: whiteboard || undefined, // ğŸ†• ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡ã¨ã—ã¦æ¸¡ã™ã‚ˆï¼ğŸ’…
            temperature: agent.temperature ?? 0.7,
        });

        // 6. çµæœã‚’è¿”ã™
        return {
            success: true,
            status: "in_progress",
            messages: [{
                content: response.content,
                agent_id: agent.id,
                agent_name: agent.name,
                agent_role: agent.role,
                agent_avatar_url: agent.avatar_url,
                usage: response.usage,
            }],
            total_usage: response.usage,
        };
    } catch (error: any) {
        console.error("LLM Execution Error:", error);
        return {
            success: false,
            status: "in_progress",
            messages: [],
            error: `LLM call failed: ${error.message || "Unknown error"}`,
        };
    }
}

/**
 * ğŸ†• è¤‡æ•°ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåŒæ™‚ã«ç™ºè¨€ã™ã‚‹
 */
async function handleParallelSpeak(
    step: ParallelSpeakStep,
    context: ExecutionContext
): Promise<ExecutionResult> {
    const { agents, meeting, messages, whiteboard, workflow } = context;

    try {
        const promises = step.agent_ids.map(async (agentId) => {
            const agent = agents.get(agentId);
            if (!agent) {
                throw new Error(`Agent not found: ${agentId}`);
            }

            const style = await getOutputStyle(agent.style_id);
            if (!style) {
                throw new Error(`Output style not found for agent ${agent.name}`);
            }

            const startPrompt = meeting.start_prompt_override || workflow.start_prompt;
            const systemPrompt = buildSystemPrompt(agent, style, startPrompt);
            const userMessage = buildUserMessage(
                meeting.topic,
                messages, // ğŸ’… whiteboardã‚’å‰Šé™¤
                agent.role
            );

            const response = await callLLM(userMessage, {
                provider: agent.llm as "openai" | "anthropic" | "google",
                model: normalizeModelId(agent.model),
                systemPrompt: systemPrompt,
                cacheableContext: whiteboard || undefined, // ğŸ†• ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡ã¨ã—ã¦æ¸¡ã™ã‚ˆï¼ğŸ’…
                temperature: agent.temperature ?? 0.7,
            });

            return {
                content: response.content,
                agent_id: agent.id,
                agent_name: agent.name,
                agent_role: agent.role,
                agent_avatar_url: agent.avatar_url,
                usage: response.usage,
            };
        });

        const results = await Promise.allSettled(promises);

        // æˆåŠŸã—ãŸçµæœã ã‘ã‚’æŠ½å‡ºğŸ’…
        const successfulResults = results
            .filter((res): res is PromiseFulfilledResult<any> => res.status === "fulfilled")
            .map(res => res.value as StepResult);

        // å¤±æ•—ã—ãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ãƒ­ã‚°å‡ºåŠ›
        const failedCount = results.filter(res => res.status === "rejected").length;
        if (failedCount > 0) {
            console.warn(`âš ï¸ ParallelSpeak: ${failedCount}äººã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒç™ºè¨€ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ä¼šè­°ã‚’ç¶™ç¶šã—ã¾ã™ã€‚ğŸ›¡ï¸`);
        }

        if (successfulResults.length === 0 && (step as ParallelSpeakStep).agent_ids.length > 0) {
            throw new Error("å…¨å“¡ã®ç™ºè¨€ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ğŸ’…ğŸ’¦");
        }

        // ãƒˆãƒ¼ã‚¿ãƒ«ä½¿ç”¨é‡ã‚’è¨ˆç®—
        const totalUsage = successfulResults.reduce(
            (acc, res) => ({
                input_tokens: acc.input_tokens + (res.usage?.input_tokens || 0),
                output_tokens: acc.output_tokens + (res.usage?.output_tokens || 0),
            }),
            { input_tokens: 0, output_tokens: 0 }
        );

        return {
            success: true,
            status: "in_progress",
            messages: successfulResults,
            total_usage: totalUsage,
        };
    } catch (error: any) {
        console.error("Parallel LLM Execution Error:", error);
        return {
            success: false,
            status: "in_progress",
            messages: [],
            error: `Parallel LLM call failed: ${error.message || "Unknown error"}`,
        };
    }
}

/**
 * ğŸ†• è­°é•·ãŒã“ã‚Œã¾ã§ã®è­°è«–ã‚’ã¾ã¨ã‚ã‚‹
 */
async function handleSummary(
    step: SummaryStep,
    context: ExecutionContext
): Promise<ExecutionResult> {
    const { agents, meeting, messages, whiteboard, workflow } = context;

    // ğŸ†• ã‚µãƒãƒªãƒ¼æ‹…å½“ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å–å¾—ï¼ˆä¼šè­°ã§ã®ä¸Šæ›¸ãã‚’æœ€å„ªå…ˆï¼ï¼‰
    const summaryAgentId = meeting.summary_agent_id || step.agent_id;
    const agent = agents.get(summaryAgentId);
    if (!agent) {
        return {
            success: false,
            status: "in_progress",
            messages: [],
            error: `Summary agent not found: ${summaryAgentId}`,
        };
    }

    // ğŸ†• å‡ºåŠ›ã‚¹ã‚¿ã‚¤ãƒ«å–å¾—ï¼ˆè­°é•·ã‚‚è‡ªåˆ†ã‚‰ã—ãï¼ğŸ’…ï¼‰
    const style = await getOutputStyle(agent.style_id);
    if (!style) {
        return {
            success: false,
            status: "in_progress",
            messages: [],
            error: `Output style not found for agent: ${agent.name}`,
        };
    }

    // ğŸ†• ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼ã«ã‚¹ã‚¿ã‚¤ãƒ«ã‚’æ¸¡ã™ã‚ˆï¼âœ¨
    const endPrompt = meeting.end_prompt_override || workflow.end_prompt;
    const systemPrompt = buildSummarySystemPrompt(agent, style, endPrompt);
    const userMessage = buildSummaryUserMessage(meeting.topic, messages); // ğŸ’… whiteboardã‚’å‰Šé™¤

    try {
        // ğŸ†• ã‚µãƒãƒªãƒ¼ä½œæˆã¯ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å¤§é‡ã«ä½¿ã†ã‹ã‚‰ã€16384ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§é–‹æ”¾ï¼ğŸš€
        const response = await callLLM(userMessage, {
            provider: agent.llm as "openai" | "anthropic" | "google",
            model: normalizeModelId(agent.model),
            systemPrompt: systemPrompt,
            cacheableContext: whiteboard || undefined, // ğŸ†• ã‚­ãƒ£ãƒƒã‚·ãƒ¥å¯¾è±¡ã¨ã—ã¦æ¸¡ã™ã‚ˆï¼ğŸ’…
            temperature: agent.temperature ?? 0.7,
            maxTokens: 16384, // é™ç•Œçªç ´ï¼ğŸ’…
        });

        return {
            success: true,
            status: "completed", // ã‚µãƒãƒªãƒ¼ãŒå‡ºãŸã‚‰ä¼šè­°çµ‚äº†ï¼ğŸ
            messages: [{
                content: response.content,
                agent_id: agent.id,
                agent_name: agent.name,
                agent_role: agent.role,
                agent_avatar_url: agent.avatar_url,
                usage: response.usage,
            }],
            total_usage: response.usage,
        };
    } catch (error: any) {
        console.error("Summary LLM Execution Error:", error);
        return {
            success: false,
            status: "in_progress",
            messages: [],
            error: `Summary generation failed: ${error.message || "Unknown error"}`,
        };
    }
}

/**
 * ğŸ†• ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ä»‹å…¥ã‚’å¾…ã¤ï¼ˆä¸€æ™‚åœæ­¢ï¼‰
 */
async function handleUserIntervention(
    step: UserInterventionStep,
    _context: ExecutionContext
): Promise<ExecutionResult> {
    // ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯LLMã¯å‘¼ã°ãšã€å˜ã«ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ "waiting" ã«ã—ã¦è¿”ã™ã ã‘ï¼
    // å®Ÿéš›ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦ã€Œå¾…æ©Ÿä¸­ã€ã¨ã„ã†ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡ºã™ã‚ˆã€‚

    return {
        success: true,
        status: "waiting", // ã“ã‚ŒãŒè¶…å¤§äº‹ï¼ğŸ’…
        messages: [{
            content: step.label || "ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›ã‚’å¾…ã£ã¦ã„ã¾ã™... ãƒ›ãƒ¯ã‚¤ãƒˆãƒœãƒ¼ãƒ‰ã‚’æ›´æ–°ã—ã¦ãã ã•ã„ã€‚ğŸ’…âœ¨",
            agent_id: "system",
            agent_name: "ã‚·ã‚¹ãƒ†ãƒ ",
            agent_role: "system", // ğŸ†• è¿½åŠ 
        }],
    };
}

// ==========================================
// å±¥æ­´ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼
// ==========================================

/**
 * ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å±¥æ­´ã‚’æ§‹é€ åŒ–ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
 */
function formatMessageHistory(messages: Message[]): string {
    if (messages.length === 0) {
        return "ï¼ˆã¾ã ç™ºè¨€ã¯ã‚ã‚Šã¾ã›ã‚“ï¼‰";
    }

    return messages.map(m => {
        // å¾Œæ–¹äº’æ›æ€§ï¼šå¤ã„ãƒ‡ãƒ¼ã‚¿ã«ã¯agent_role, step_numberãŒãªã„å¯èƒ½æ€§
        const role = m.agent_role || "ä¸æ˜";
        const step = m.step_number !== undefined ? m.step_number : "?";

        return `ã€ç™ºè¨€è€…ã€‘${m.agent_name}
ã€ç™ºè¨€è€…ã®å½¹å‰²ã€‘${role}
ã€ç™ºè¨€ã‚¹ãƒ†ãƒƒãƒ—ã€‘${step}
ã€å†…å®¹ã€‘
${m.content}`;
    }).join("\n\n---\n\n");
}

// ==========================================
// ã‚µãƒãƒªãƒ¼ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ“ãƒ«ãƒ€ãƒ¼
// ==========================================

/**
 * ã‚µãƒãƒªãƒ¼ç”Ÿæˆç”¨ã®ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰
 */
function buildSummarySystemPrompt(
    agent: Agent,
    style: OutputStyle,
    endPrompt: string
): string {
    return `ã‚ãªãŸã¯ã€Œ${agent.name}ã€ã¨ã„ã†åå‰ã®ä¼šè­°å‚åŠ è€…ã§ã™ã€‚
ä»Šå›ã€ã‚ãªãŸã¯ä¼šè­°ã®ã¾ã¨ã‚å½¹ã‚’æ‹…å½“ã—ã¾ã™ã€‚

## ã‚ãªãŸã®æ€§æ ¼ãƒ»è¨­å®š
${agent.persona}

${agent.prompt ? `## è¿½åŠ ã®æŒ‡ç¤º\n${agent.prompt}` : ""}

## ã¾ã¨ã‚ä½œæˆã®æŒ‡ç¤º
${endPrompt}

## å‡ºåŠ›å½¢å¼ãƒ»ã‚¹ã‚¿ã‚¤ãƒ«
${style.prompt_segment}

## å‡ºåŠ›ã®é•·ã•ã«ã¤ã„ã¦
å‡ºåŠ›ã®é•·ã•ã«åˆ¶é™ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚è­°è«–ã®å†…å®¹ã‚’ç¶²ç¾…çš„ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

---
ä¸Šè¨˜ã®è¨­å®šã‚’éµå®ˆã—ã¦ã€è­°è«–ã®ã¾ã¨ã‚ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚`;
}

/**
 * ã‚µãƒãƒªãƒ¼ç”Ÿæˆç”¨ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ§‹ç¯‰
 */
function buildSummaryUserMessage(
    topic: string,
    messages: Message[]
): string {
    const history = formatMessageHistory(messages);

    return `## ä¼šè­°ã®è­°é¡Œ
${topic}

## ã“ã‚Œã¾ã§ã®ä¼šè­°è¨˜éŒ²
${history}

---
ä¸Šè¨˜ã®è­°è«–ã‚’è¸ã¾ãˆã¦ã€ã¾ã¨ã‚ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚`;
}

// ==========================================
// ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰ï¼ˆã‚®ãƒ£ãƒ«ã®ã“ã ã‚ã‚Šï¼ğŸ’…ï¼‰
// ==========================================

function buildSystemPrompt(
    agent: Agent,
    style: OutputStyle,
    startPrompt: string
): string {
    return `ã‚ãªãŸã¯ã€Œ${agent.name}ã€ã¨ã„ã†åå‰ã®ä¼šè­°å‚åŠ è€…ã§ã™ã€‚

## ã‚ãªãŸã®å½¹å‰²
${agent.role}

## ã‚ãªãŸã®æ€§æ ¼ãƒ»è¨­å®š
${agent.persona}

${agent.prompt ? `## è¿½åŠ ã®æŒ‡ç¤º\n${agent.prompt}` : ""}

## ä¼šè­°ã®é€²è¡Œãƒ«ãƒ¼ãƒ«
${startPrompt}

## å‡ºåŠ›å½¢å¼ãƒ»ã‚¹ã‚¿ã‚¤ãƒ«
${style.prompt_segment}

## å‡ºåŠ›ã®é•·ã•ã«ã¤ã„ã¦
å‡ºåŠ›ã®é•·ã•ã«åˆ¶é™ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚è­°è«–ã«å¿…è¦ãªå†…å®¹ã‚’éä¸è¶³ãªãè¨˜è¿°ã—ã¦ãã ã•ã„ã€‚

---
ä¸Šè¨˜ã®è¨­å®šã‚’éµå®ˆã—ã¦ã€è­°è«–ã«è²¢çŒ®ã—ã¦ãã ã•ã„ã€‚`;
}

function buildUserMessage(
    topic: string,
    messages: Message[],
    role: string
): string {
    // ğŸ†• æ–°ã—ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ãƒ¼ã‚’ä½¿ç”¨
    const history = formatMessageHistory(messages);

    return `## ä¼šè­°ã®è­°é¡Œ
${topic}

## è­°è«–ã®å±¥æ­´
${history}

---
ã‚ãªãŸã¯ã€Œ${role}ã€ã¨ã—ã¦ã€ã“ã®è­°è«–ã®æµã‚Œã‚’è¸ã¾ãˆã€æ¬¡ã«è¿°ã¹ã‚‹ã¹ãæ„è¦‹ã‚„è³ªå•ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚
å‡ºåŠ›ã®é•·ã•ã«åˆ¶é™ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å¿…è¦ã«å¿œã˜ã¦è©³ç´°ã«è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚`;
}
