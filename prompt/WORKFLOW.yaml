workflow:
  last_updated: "2026-02-03 13:10:00"
  last_session_summary: |
    【今回のセッションでやったこと】
    - LLMモデル名の2026年最新仕様への対応（Gemini 3, Claude 4.5, GPT-5）
    - 全プロバイダー（Anthropic, Google, OpenAI）を対象とした Prompt Caching の実装
    - whiteboardをプロンプトから分離し、cacheableContextとしてLLMへ渡すリファクタリングの実施
    - 統合キャッシュログシステム（LLM Cache Hit表示）の導入による可視化
    
    【次にやること】
    - 実際の多人数会議でのキャッシュヒット率の定量的評価
    - 長時間会議（コンテキスト肥大化時）のメモリ管理とTTLの最適化検討
  
  progress:
    current_phase:
      number: 7
      name: "Cost Efficiency & Prompt Caching Optimization"
      status: "in_progress"
    
    completed_phases:
      - phase: 1
        name: "Type Definitions & DB Design"
        completed_at: "2026-02-02"
      - phase: 2
        name: "Backend Implementation"
        completed_at: "2026-02-02"
      - phase: 3
        name: "Frontend Implementation"
        completed_at: "2026-02-02"
      - phase: 4
        name: "Integration Testing"
        completed_at: "2026-02-02"
      - phase: 5
        name: "LLM Adjustments Integration"
        completed_at: "2026-02-02"
      - phase: 6
        name: "Advanced Content Engineering"
        completed_at: "2026-02-03"

  decisions:
    adopted:
      - id: "D008"
        decision: "Prompt Caching用の構造的レイヤリングを採用"
        reason: "whiteboard（大規模コンテキスト）を最上位に配置し、エージェント人格や履歴を分離することでキャッシュ再利用率を最大化するため"
      - id: "D009"
        decision: "Gemini 3/Flash等のモデル名正規化ロジックの導入"
        reason: "2026年現在のAPI仕様（-preview必須化等）に合わせ、ユーザー指定の簡略名と実API名の乖離を吸収するため"

  features:
    in_progress:
      - id: "F-Prompt-Caching"
        name: "Prompt Caching Optimization"
        status: "in_progress"
    
    planned:
      - id: "F-Cost-Dashboard"
        name: "LLM Cost & Cache Analytics Dashboard"
        status: "planned"

  file_structure:
    created: []
    to_modify:
      - "src/lib/llm.ts"
      - "src/lib/workflow-engine.ts"

  cautions:
    - "AnthropicのキャッシュはTTLが5分〜のため、短時間の連続発言では有効だが、長時間放置した後の再開時は初回書込が発生することに留意。"
